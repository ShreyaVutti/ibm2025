# -*- coding: utf-8 -*-
"""Copy of 4 resume parsing - working code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hKFTIuTOjUCdvYX1GuwAR07_Jl8ISFUL
"""

!pip install humanfriendly

!pip install rake-nltk

!pip install --upgrade pip
!pip install docling

!pip install ibm-watson

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab')

import os

if os.getenv("COLAB_RELEASE_TAG"):
   print("Running in Colab")
   RUNNING_IN_COLAB = True
else:
   print("NOT in Colab")
   RUNNING_IN_COLAB = False

if RUNNING_IN_COLAB:
  ! pip install-default-timeout=100 \
  data-prep-toolkit-transforms [all]==1.0.0\
  humanfriendly

import os

if os.getenv("COLAB_RELEASE_TAG"):
  print("Running in Colab")
  RUNNING_IN_COLAB = True
else:
  print("NOT in Colab")
  RUNNING_IN_COLAB = False

import os, sys
import shutil

if RUNNING_IN_COLAB:
  input_dir = "input"
  shutil.os.makedirs(input_dir, exist_ok=True)
else:
  input_dir = "../../data-files/pdf-processing-1/"

output_dir = "output"
output_pdf2pq_dir = os.path.join (output_dir, '01_pdf2pq_out')
output_docid_dir = os.path.join (output_dir, '02_docid_out')
output_exact_dedupe_dir = os.path.join (output_dir, '03_exact_dedupe_out')
output_fuzzy_dedupe_dir = os.path.join (output_dir, '04_fuzzy_dedupe_out')
output_doc_quality_dir = os.path.join (output_dir, '05_doc_quality_out')
output_final_dir = os.path.join (output_dir, 'output_final')
shutil.rmtree (output_dir, ignore_errors=True)
shutil.os.makedirs(output_dir, exist_ok=True)
print(" cleared output directory")

import os
import requests
from humanfriendly import format_size
import pandas as pd
import glob

## Reads parquet files in a folder into a pandas dataframe

def read_parquet_files_as_df (parquet_dir):
  parquet_files = glob.glob (f'{parquet_dir}/*.parquet')
  #read each parquet file into a DataFrame and store in a list
  dfs= [pd.read_parquet (f) for f in parquet_files]
  dfs = [df for df in dfs if not df.empty] # filter out empty dataframes
  # Concatenate all DataFrames into a single DataFrame
  if len(dfs) > 0:
    data_df = pd.concat(dfs, ignore_index=True)
    return data_df
  else:
    return pd.DataFrame() # return empty df

import requests
import os

def format_size(bytes):
    """Converts bytes to a human-readable format."""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes < 1024:
            return f"{bytes:.2f} {unit}"
        bytes /= 1024
    return f"{bytes:.2f} PB"

def download_file(url, local_file, chunk_size=1024*1024):
    """
    Downloads a remote URL to a local file.

    Args:
        url (str): The remote URL to download from.
        local_file (str): The local file path where the content should be saved.
        chunk_size (int): The size in bytes of each chunk. Defaults to 1MB.

    Returns:
        str: The path to the downloaded local file.
    """
    response = requests.get(url, stream=True)
    response.raise_for_status()  # Raise an exception for HTTP errors

    with open(local_file, 'wb') as f:
        for chunk in response.iter_content(chunk_size=chunk_size):
            if chunk:  # Skip keep-alive chunks
                f.write(chunk)

    file_size = format_size(os.path.getsize(local_file))
    print(f"\n{local_file} ({file_size}) downloaded successfully.")

    return local_file

from google.colab import files

uploaded = files.upload()  # This opens a file picker in the browser
pdf_path = list(uploaded.keys())[0]  # Get the name of the uploaded file

print(f"Selected PDF: {pdf_path}")

from docling.document_converter import DocumentConverter
import pyarrow.parquet as pq
import pandas as pd
import pyarrow as pa

# 1. Convert PDF to a DoclingDocument
converter = DocumentConverter()
result = converter.convert(pdf_path)  # This returns a ConversionResult
doc = result.document  # This is the actual DoclingDocument

# 2. Extract page-by-page text
records = []
for page in doc.texts:  # Now this should work
    page_number = page.prov[0].page_no if page.prov else None

    text = page.text
    records.append({"filename": "ibm_resume_input.pdf", "page": page_number, "text": text})

# 3. Build a DataFrame & write to Parquet
df = pd.DataFrame(records)
table = pa.Table.from_pandas(df)
pq.write_table(table, "resume_output.parquet")

# 🔍 Preview
print(df.head(15))

from rake_nltk import Rake
rake = Rake()

def extract_keywords(text):
    rake.extract_keywords_from_text(text)
    return rake.get_ranked_phrases()[:5]  # top 5 keywords

df["keywords"] = df["text"].apply(extract_keywords)
print(df[["page", "keywords"]])

import spacy
import re

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

def mask_pii_advanced(text):
    # -------- Regex-based masking --------
    # Email
    text = re.sub(r'\b[\w\.-]+@[\w\.-]+\.\w+\b', '[EMAIL]', text)

    # Phone number (international + local formats)
    text = re.sub(r'(\+?\d{1,3}[-.\s]?)?\(?\d{2,5}\)?[-.\s]?\d{2,5}[-.\s]?\d{3,5}', '[PHONE]', text)

    # LinkedIn
    text = re.sub(r'https?://(www\.)?linkedin\.com/in/[A-Za-z0-9\-_%]+', '[LINKEDIN]', text, flags=re.I)

    # GitHub
    text = re.sub(r'https?://(www\.)?github\.com/[A-Za-z0-9\-_%]+', '[GITHUB]', text, flags=re.I)

    # -------- NER-based name masking --------
    doc = nlp(text)
    for ent in doc.ents:
        if ent.label_ == "PERSON" and len(ent.text.strip()) > 1:
            text = text.replace(ent.text, '[NAME]')

    # -------- Backup: Keyword-based name masking --------
    name_keywords = ['name', 'full name', 'candidate name']
    for keyword in name_keywords:
        pattern = rf"{keyword}[:\-]?\s*([A-Z][a-z]+\s[A-Z][a-z]+)"
        text = re.sub(pattern, lambda m: keyword + ': [NAME]', text, flags=re.I)

    return text

df['masked_text'] = df['text'].apply(mask_pii_advanced)
print(df[['page', 'masked_text']])

resume_text = "\n".join(df['masked_text'].tolist())
print(resume_text[:1000])  # Just preview first 1000 characters

from ibm_watson import NaturalLanguageUnderstandingV1
from ibm_cloud_sdk_core.authenticators import IAMAuthenticator
from ibm_watson.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions

# Paste your IBM Watson credentials here
WATSON_API_KEY = "6MVbF27AZ0E80RArEC1o4tnrvIeMRdfhiI6Plov--pBE"
WATSON_URL = "https://api.au-syd.natural-language-understanding.watson.cloud.ibm.com/instances/d7f3f8f6-6373-4ce5-8d7c-fe3c2cbe55f4"

# Authenticate
authenticator = IAMAuthenticator(WATSON_API_KEY)
nlu = NaturalLanguageUnderstandingV1(
    version='2022-04-07',
    authenticator=authenticator
)
nlu.set_service_url(WATSON_URL)

def extract_resume_info_with_watson(resume_text):
    try:
        response = nlu.analyze(
            text=resume_text,
            features=Features(
                entities=EntitiesOptions(limit=50),
                keywords=KeywordsOptions(limit=50)
            )
        ).get_result()

        # Extract keywords and entities
        keywords = [k['text'] for k in response.get("keywords", [])]
        entities = response.get("entities", [])

        # Define a basic skill keywords list (can be expanded)
        skill_keywords = [
    # Programming Languages
    'Python', 'Java', 'C', 'C++', 'C#', 'JavaScript', 'TypeScript', 'R', 'Go', 'Kotlin', 'Ruby', 'Swift',
    # Web & App Development
    'HTML', 'CSS', 'React', 'Angular', 'Vue', 'Django', 'Flask', 'Node.js', 'Express', 'Bootstrap',
    # Databases
    'MySQL', 'PostgreSQL', 'MongoDB', 'SQLite', 'Oracle', 'Firebase',
    # Cloud Platforms & DevOps
    'AWS', 'Azure', 'GCP', 'Docker', 'Kubernetes', 'Git', 'GitHub', 'CI/CD', 'Jenkins',
    # AI / ML / Data Science
    'Machine Learning', 'Deep Learning', 'TensorFlow', 'Keras', 'Scikit-learn', 'PyTorch',
    'Numpy', 'Pandas', 'Matplotlib', 'OpenCV', 'NLTK', 'SpaCy', 'HuggingFace',
    # IoT & Embedded
    'Arduino', 'Raspberry Pi', 'IoT', 'ESP32', 'Arduino IDE', 'MQTT', 'Tinkercad',
    # Networking & Cybersecurity
    'Cisco Packet Tracer', 'Wireshark', 'Nmap', 'Burp Suite', 'Cybersecurity', 'Networking',
    # Tools & Platforms
    'VS Code', 'Jupyter Notebook', 'Git', 'GitHub', 'Figma', 'Blender', 'Unity',
    # Data & Analytics
    'Excel', 'Power BI', 'Tableau', 'SQL', 'Big Data', 'Hadoop', 'Spark',
    # Operating Systems & Environments
    'Linux', 'Ubuntu', 'Windows', 'Shell Scripting', 'Bash'
]

        # Match skills
        skills = list({
    kw for kw in keywords
    if any(skill.lower() in kw.lower() or kw.lower() in skill.lower()
           for skill in skill_keywords)
})

        # Extract experience from entities (company names, roles, durations)
        experience = []
        for e in entities:
            if e.get("type") in ["Company", "JobTitle", "Date"]:
                experience.append(e["text"])

        # Basic project extraction by matching lines from resume text
        projects = []
        for line in resume_text.splitlines():
            if "project" in line.lower():
                projects.append(line.strip())

        return {
            "skills": skills,
            "experience": list(set(experience)),  # remove duplicates
            "projects": projects
        }

    except Exception as e:
        return {"error": str(e)}

parsed_info = extract_resume_info_with_watson(resume_text)
print(parsed_info)

